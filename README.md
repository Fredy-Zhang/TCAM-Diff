# TCAM-Diff: Triplane-Aware Cross-Attention Medical Diffusion Model

[![AAAI 2025](https://img.shields.io/badge/AAAI-2025-blue)](https://ojs.aaai.org/index.php/AAAI/article/view/34433)
[![DOI](https://img.shields.io/badge/DOI-10.1609%2Faaai.v39i21.34433-blue)](https://doi.org/10.1609/aaai.v39i21.34433)

Official implementation of **TCAM-Diff**, a novel 3D medical image generation model presented at [AAAI 2025](https://ojs.aaai.org/index.php/AAAI/article/view/34433).

## Paper

**[TCAM-Diff: Triplane-Aware Cross-Attention Medical Diffusion Model](https://ojs.aaai.org/index.php/AAAI/article/view/34433)**  
Zhenkai Zhang, Krista A. Ehinger, Tom Drummond  
*The University of Melbourne*  
*Proceedings of the AAAI Conference on Artificial Intelligence, 39(21), 22732â€“22740 (2025)*

<p align="center">
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/34433/36588">ðŸ“„ Paper (PDF)</a>
  &nbsp;&nbsp;|&nbsp;&nbsp;
  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/34433/40118">ðŸ“„ Poster</a>
</p>

---

## Abstract

We introduce TCAM-Diff, a novel 3D medical image generation model that reduces the memory requirements to encode and generate high-resolution 3D data. This model utilizes a decoder-only autoencoder method to learn triplane representation from dense volume and leverages generalization operations to prevent overfitting. Subsequently, it uses a triplane-aware cross-attention diffusion model to learn and integrate these features effectively. Furthermore, the features generated by the diffusion model can be rapidly transformed into 3D volumes using a pre-trained decoder module. Our experiments on three different scales of medical datasetsâ€”**BrainTumour** (128Ã—128Ã—128), **Pancreas** (256Ã—256Ã—256), and **Colon** (512Ã—512Ã—512)â€”demonstrated outstanding results. We utilized MSE and SSIM to evaluate reconstruction quality and leveraged the Wasserstein Generative Adversarial Network (W-GAN) critic to assess generative quality. Comparisons to existing approaches show that our method gives better reconstruction and generation results than other encoder-decoder methods with similar-sized latent spaces.

---

## Method Overview

- **Triplane representation**: Decoder-only autoencoder for memory-efficient encoding of 3D volumes
- **Generalization operations**: Mitigate overfitting on medical data
- **Triplane-aware cross-attention diffusion**: Effectively learns and integrates triplane features
- **Fast 3D decoding**: Pre-trained decoder converts diffusion outputs to 3D volumes

---

## Datasets

| Dataset | Resolution | Description |
|---------|------------|-------------|
| BrainTumour | 128Ã—128Ã—128 | Brain tumour imaging |
| Pancreas | 256Ã—256Ã—256 | Pancreas CT scans |
| Colon | 512Ã—512Ã—512 | Colon imaging |

---

## Requirements

- Python 3.8+
- PyTorch 2.0+
- MONAI
- Additional dependencies in [environment.yml](environment.yml)

## Installation

**Via conda:**

```bash
conda env create -f environment.yml
conda activate diffusion
```

**Or with pip:**

```bash
pip install -r requirements.txt
```

---

## Configuration

### Triplane configs (`triplanes/configs/`)

| Config | Dataset | Resolution |
|--------|---------|------------|
| `brain.yml` | Brain dataset | 128Â³ ROI |
| `colon.yml` | Colon dataset | 512Â³ ROI |
| `pancreas.yml`, `vessel.yml` | Other medical datasets | â€” |

Key options: `roi_size`, `batch_size`, `n_epochs`, `optim.loss` (smoothl1, l1, l2), loss weights (`weight_ssim`, `weight_p`, `weight_tv`).

### Diffusion config (`diffusion/configs/triplanes.yml`)

| Option | Description |
|--------|-------------|
| `data.channels` | Triplane feature channels (e.g., 42) |
| `data.image_size` | Triplane resolution (e.g., 64) |
| `diffusion.*` | Beta schedule, timesteps |
| `embedding.pretrained` | Path to triplane checkpoint |
| `embedding.out_vol` | If true, decode to 3D volume during sampling |

---

## Training

### Stage 1: Triplane Decoder

Train the triplane encoder-decoder to learn compact 3D representations. This produces `features.pth` used by the diffusion stage.

**Standard resolution (e.g., 128Â³):**

```
python triplanes_train.py --configs brain.yml --source_data /path/to/brain/data
```

**Full resolution with subsampled voxels (e.g., 512Â³):**

```
python train_decoder_large_triplane.py --configs colon.yml --source_data /path/to/colon/data
```

**MultiTriplane with SSIM & perceptual loss:**

```
python train_decoder_triplane.py --configs brain.yml
```

Optional flags: `--resume_training`, `--gpu_id 0`, `--mlp_dim 128 128 128 128`.

**Wandb (optional):** Logging is disabled by default for publication. To enable:

```
python train_decoder_triplane.py --configs brain.yml --wandb_project my_project
# Or use environment variables: WANDB_PROJECT, WANDB_ENTITY
# To explicitly disable: --no_wandb
```

### Stage 2: Diffusion Model

Train the diffusion model on triplane features. Ensure `diffusion/configs/triplanes.yml` points to your pretrained decoder and `features.pth`:

```yaml
embedding:
  pretrained: "logs/ckpt.pth"
  out_vol: true

data:
  feats_path: "logs/features.pth"
```

Run training:

```
python main.py --config triplanes.yml --doc my_experiment
```

Resume training:

```
python main.py --config triplanes.yml --doc my_experiment --resume_training
```

### Stage 3: Sampling

Generate new 3D volumes:

```
python main.py --config triplanes.yml --doc my_experiment --sample
```

Outputs are written to `exp/images/` by default. Use `--image_folder` to override.

---

## Evaluation

After sampling, volumes are saved as NumPy arrays. Use your preferred metrics (e.g., SSIM, Dice) on the generated volumes.

---

## Acknowledgments

We thank The University of Melbourne for supporting this research. This work was presented at AAAI Technical Track on Machine Learning VII (AAAI-25).

---

## License

This project is licensed under the Apache License 2.0 â€” see the [LICENSE](LICENSE) file for details.

---

## Citation

If you use this code or find our work useful, please cite:

```bibtex
@inproceedings{zhang2025tcamdiff,
  title={TCAM-Diff: Triplane-Aware Cross-Attention Medical Diffusion Model},
  author={Zhang, Zhenkai and Ehinger, Krista A. and Drummond, Tom},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={21},
  pages={22732--22740},
  year={2025}
}
```
